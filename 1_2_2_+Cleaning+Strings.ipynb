{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.2.2.+Cleaning+Strings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtmWKqFz8YgkcY3fOqFtV7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbinB/Text-Analytics-NLP/blob/main/1_2_2_%2BCleaning%2BStrings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcEXeJny9C8i"
      },
      "source": [
        "# Lecture Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3N6XEwx9Iv_"
      },
      "source": [
        "<h2>Automating Experiments</h2>\n",
        "\n",
        "**Pipelines allow us to automate our experiments from data collection all the way to visualization.**\n",
        "\n",
        "Let’s say we want to monitor corruption in societies and governments. The more corruption there is, we think, the more people will be talking about corruption in news articles and tweets. So we use text analytics to categorize new articles and new tweets: how many people are talking about corruption today? Over time, this gives us a dashboard to monitor corruption. We can ask whether there is a statistically significant change against our expected rate. This kind of application is a good example of why we need a pipeline: to be effective, we have to monitor talk about corruption continuously. The analysis must be automated from start to finish. Automated pipelines are best practice because they allow reproducible and streaming analytics: every step along the way is performed without human intervention. This means that every choice is documented in the code and easy to change.\n",
        "\n",
        "The main idea in this module is that digital documents are stored as strings. This is a kind of data type in a programming language. But humans don’t view language as strings; we think about language as a sequence of individual words. And AI can’t work with language as strings, either. AI needs to convert language into numbers.\n",
        "\n",
        "So we need a pipeline that takes a string like “The cat sat on the mat” and turns it into something that looks like actual language: “the”, “cat”, “sat”, “on”, “the”, “mat”.  Then we need to make that language work for a machine: we have to convert it into a numeric representation. This module takes us from input (digital text) to a linguistic representation to a machine representation.\n",
        "\n",
        "* **Indexing**. Strings are ordered. This means each character has a particular place. Look at the string “the cat sat on the mat.” The first character is “t” and the seventh character is also “t”. This index includes every character, including spaces and punctuation. You’ll see why this matters in the labs.\n",
        "\n",
        "* **Punctuation**. Now we have to remove or separate punctuation. This allows us to identify individual words without punctuation getting in the way. For example, “cat” and “cat,” and “cat:” are different strings to a machine. But they aren’t different words to a human.\n",
        "\n",
        "* **Symbols and Numbers**. Texts have a lot of non-linguistic information. For example, if we want to know what language a tweet is written in, we probably want to get rid of URLs and hashtags. If we want to know what a web page is about, we probably want to get rid of email addresses. These parts of a text just create noise that gets in our way.\n",
        "\n",
        "* **Letter Case**. Are “cat” and “Cat” and “CAT” the same word? In most cases, yes. But these are different strings. So we standardize case and characters as part of our pipeline, too.\n",
        "\n",
        "\n",
        "The language that we get from a string is very different from the language that we get as humans. Think about the sentence “Peter thought he had forgotten his manners.” What kind of information does a human get out of this string?\n",
        "\n",
        "**First**, we know this string has 7 words. Four of these words contain content: Peter is a specific person, thought is a verb for cognition, forgotten is another verb for cognition, and manners is a social object, a way of behaving properly. The other three words are functional: he and his just refer back to Peter and had tells us about when it happened. Humans know the string contains these individual words and, generally, what each word means, as well\n",
        "\n",
        "**Second**, we humans know that this sentence has two clauses:\n",
        "\n",
        "**[Peter thought [he had forgotten his manners] ]**\n",
        "\n",
        "In fact, the second clause is a sentence on its own. We humans also know that **[his manners]** behaves like a single unit and that **[had forgotten]** behaves like a single unit (a noun and a verb, respectively). So humans also generalize about the type of word. We know that other nouns can go in the same place that **[his manners]** goes: Peter thought he had forgotten the three oranges. But machines don’t know about nouns.\n",
        "\n",
        "**Third**, we humans know that if you forget your keys that means you’ve left them somewhere else. And if you forget your line that means you can’t remember something. But when you forget your manners you still remember how you should act and you still have your manners with you. This is important because we have just one word (“forget”) but humans know that it means something a bit different in each case. A machine doesn’t know this.\n",
        "\n",
        "For humans, we automatically divide language into words, combine words into phrases, and rule out word meanings that aren’t relevant in our context. But machines don’t do this.\n",
        "\n",
        "How do we make strings look like language? We walk through the coding details in the labs, but the basic idea is to split, chunk, clean, and disambiguate.\n",
        "\n",
        "* **Split It.** First we take that string and split it into words. For most languages we can use whitespace, but of course that doesn’t work for languages like Chinese that don’t use whitespace the way English does. And English sometimes separates one word, like White House or attorney general, into two words. We’ve just learned to spell these words with a space in the middle. But whitespace usually works just fine.\n",
        "\n",
        "* **Clean It.** Second, we have to get the text ready. Cat and cat and cat? are three different strings. But they’re just one word. So we have to remove punctuation and get all the letters into lowercase and a few things like that. The labs give you code samples to get this all done.\n",
        "\n",
        "* **Chunk It.** We might need to know where there are gaps in a sentence. Take this sentence: “The owner of the dog who kept smelling people was horribly embarrassed.” But who is smelling people, the dog or the owner? We have two options, given below. Humans always know how to read sentences like this. But machines don’t.\n",
        "\n",
        "<br>\n",
        "(1) [The owner [of the dog] who kept smelling people]\n",
        "\n",
        "(2) [The owner [of the dog who kept smelling people]\n",
        "<br>\n",
        "\n",
        "\n",
        "* **Disambiguating.** Think of a simple joke: “Smoking might kill you and bacon might kill you, but I heard that smoking bacon actually cures it.” This is funny (if it is), because cure means two different things. There’s just one string (to a machine) but there are actually two different words (to a human). We usually know which particular sense is meant. We’ll pick this up in a later section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxUCvdLg9Dcp"
      },
      "source": [
        "# Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb6uzL9u9SPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc837825-2876-4208-f78a-a594c1b35b1c"
      },
      "source": [
        "# if you are running these labs in CoLab, you will first need to mount the drive and \n",
        "# copy text_analitics.py to path \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QfUtbEV9Si9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917ed6f1-999b-4aa4-9b36-df35c0cb1b10"
      },
      "source": [
        "###Add text_analytics.py to path \n",
        "!cp \"/content/drive/My Drive/Colab Notebooks/CourseWork/Text Analytics and Natural Language Processing/text_analytics.py\" .\n",
        "print(\"Done!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzMC7zDN9Dxo"
      },
      "source": [
        "# Lecture Lab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlV7nnUQfwJT"
      },
      "source": [
        "Welcome to our first lab of Module 2! In this lab we're going to do some work with strings.\n",
        "\n",
        "So let's start by loading up our environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLJbB2t3fwJV",
        "outputId": "5f7316e4-f38f-4865-9df0-55d58537b898"
      },
      "source": [
        "from text_analytics import text_analytics\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "ai = text_analytics()\n",
        "print(\"Done!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjWPQPKDfwJV"
      },
      "source": [
        "This time we're going to work with articles about corruption. These are lead paragraphs from *The New York Times*. Some are about corruption and some aren't. But they are drawn from the same set of countries. So we load our data set in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9wyGV-kfwJW",
        "outputId": "baff4173-2e41-46ea-901d-a1bdd489c3c3"
      },
      "source": [
        "file = \"NYT.Corruption.gz\"\n",
        "file = os.path.join(ai.data_dir, file)\n",
        "print(file)\n",
        "df = pd.read_csv(file, index_col = 0)\n",
        "print(df)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Data/NYT.Corruption.gz\n",
            "         Country       Class                                               Text\n",
            "0      Argentina       Other  Secretary of State Madeleine Albright, visitin...\n",
            "1      Argentina  Corruption  Argentine Pres Fernando de la Rua denies recen...\n",
            "2      Argentina       Other  British Telecommunications PLC plans to invest...\n",
            "3      Argentina       Other  Shareholders clear $650 million rescue package...\n",
            "4      Argentina       Other  United States International Trade Commission r...\n",
            "...          ...         ...                                                ...\n",
            "19310  Venezuela       Other  Representative Ilhan Omar, a Minnesota Democra...\n",
            "19311  Venezuela       Other  Secretary of State Mike Pompeo made his first ...\n",
            "19312  Venezuela       Other  President Nicolás Maduro has long enjoyed the ...\n",
            "19313  Venezuela       Other  Secretary of State Mike Pompeo discussed diffe...\n",
            "19314  Venezuela       Other  Juan Guaidó, the Venezuelan opposition leader,...\n",
            "\n",
            "[19315 rows x 3 columns]\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qdGK6xDfwJW"
      },
      "source": [
        "We're working with *strings* today. So we'll grab one at random from the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb4ofaVZfwJW",
        "outputId": "4fbcc303-3278-4a20-e4ce-bbbde06e8fd2"
      },
      "source": [
        "line = ai.print_sample(df)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The Red Cross sent in their first shipment of medical supplies and hopes to distribute them without political intervention. The delay cost an untold number of lives.']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNJ1YKOCfwJX"
      },
      "source": [
        "So here we have a raw string. It doesn't tell us anything about words.\n",
        "\n",
        "Here we're going to split this string into individual words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qerNw7L1fwJX",
        "outputId": "de79c538-821e-4d28-90b9-85f8e64bb4ac"
      },
      "source": [
        "line_split = line.split()\n",
        "print(line_split)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Red', 'Cross', 'sent', 'in', 'their', 'first', 'shipment', 'of', 'medical', 'supplies', 'and', 'hopes', 'to', 'distribute', 'them', 'without', 'political', 'intervention.', 'The', 'delay', 'cost', 'an', 'untold', 'number', 'of', 'lives.']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey_oSVUAfwJX"
      },
      "source": [
        "We notice that, if there is any punctuation, it is included inside of a word. So we have a series of cleaning steps change this. Let's get a new line and try it in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6BWyJ5zfwJX",
        "outputId": "1b2d8a1b-d030-424c-d88f-dad1f16ec80d"
      },
      "source": [
        "#commenting out code from the class, because i would rather see the same line go through different transformations \n",
        "#line = ai.print_sample(df)\n",
        "print(\"line\")\n",
        "print(\"\\n\")\n",
        "#commenting out code from the class, because i would rather see the same line go through different transformations \n",
        "#line = ai.clean_wordclouds(line, stage = 1)\n",
        "line_clean_stage1 = ai.clean_wordclouds(line, stage = 1)\n",
        "print(line_clean_stage1)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "line\n",
            "\n",
            "\n",
            "['The', 'Red', 'Cross', 'sent', 'shipment', 'medical', 'supplies', 'hopes', 'distribute', 'political', 'intervention.', 'The', 'delay', 'cost', 'untold', 'number', 'lives.']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xJGBONkfwJX"
      },
      "source": [
        "This removes stopwords. These are words that are so common they dilute the content of a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psaEQ2NkfwJY",
        "outputId": "6618262f-78b9-41d8-9ee3-4968f68b3d07"
      },
      "source": [
        "#commenting out code from the class, because i would rather see the same line go through different transformations \n",
        "#line = ai.print_sample(df)\n",
        "print(line)\n",
        "print(\"\\n\")\n",
        "line_clean_stage2 = ai.clean_wordclouds(line, stage = 2)\n",
        "print(line_clean_stage2)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Red Cross sent in their first shipment of medical supplies and hopes to distribute them without political intervention. The delay cost an untold number of lives.\n",
            "\n",
            "\n",
            "['red', 'cross', 'sent', 'shipment', 'medical', 'supplies', 'hopes', 'distribute', 'political', 'intervention.', 'delay', 'cost', 'untold', 'number', 'lives.']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxc54fYffwJY"
      },
      "source": [
        "This makes everything lowercase as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ2ln4CVfwJY",
        "outputId": "cc8a9af2-ca56-469b-9c9d-04af751b432c"
      },
      "source": [
        "#commenting out code from the class, because i would rather see the same line go through different transformations \n",
        "#line = ai.print_sample(df)\n",
        "print(line)\n",
        "print(\"\\n\")\n",
        "line_clean_stage3 = ai.clean_wordclouds(line, stage = 3)\n",
        "print(line_clean_stage3)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Red Cross sent in their first shipment of medical supplies and hopes to distribute them without political intervention. The delay cost an untold number of lives.\n",
            "\n",
            "\n",
            "['red', 'cross', 'sent', 'shipment', 'medical', 'supplies', 'hopes', 'distribute', 'political', 'intervention', 'delay', 'cost', 'untold', 'number', 'lives']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnIHwxpufwJY"
      },
      "source": [
        "This has removed punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2seZnjr6fwJY",
        "outputId": "228ff7bd-3162-439e-d769-09b40b3ffc7f"
      },
      "source": [
        "#commenting out code from the class, because i would rather see the same line go through different transformations \n",
        "#line = ai.print_sample(df)\n",
        "print(line)\n",
        "print(\"\\n\")\n",
        "line_clean_stage4 = ai.clean_wordclouds(line, stage = 4)\n",
        "print(line_clean_stage4)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Red Cross sent in their first shipment of medical supplies and hopes to distribute them without political intervention. The delay cost an untold number of lives.\n",
            "\n",
            "\n",
            "['red', 'cross', 'sent', 'shipment', 'medical', 'supplies', 'hopes', 'distribute', 'political', 'intervention', 'delay', 'cost', 'untold', 'number', 'lives']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82c4sR07fwJZ"
      },
      "source": [
        "And this gets rid of any other non-linguistic material, like email addresses. So, we don't always need to use this step-by-step cleaning function. Mostly we'll use the code below to fully clean each line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fktpiVv5fwJZ",
        "outputId": "9481f489-5e4d-4470-f6a0-9065126f275c"
      },
      "source": [
        "#commenting out code from the class, because i would rather see the same line go through different transformations \n",
        "#line = ai.print_sample(df)\n",
        "print(line)\n",
        "print(\"\\n\")\n",
        "line_clean_stage5 = ai.clean_wordclouds(line, stage = 5)\n",
        "print(line_clean_stage5)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Red Cross sent in their first shipment of medical supplies and hopes to distribute them without political intervention. The delay cost an untold number of lives.\n",
            "\n",
            "\n",
            "['red', 'cross', 'sent', 'shipment', 'medical', 'supplies', 'hopes', 'distribute', 'political', 'intervention', 'delay', 'cost', 'untold', 'number', 'lives']\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiAwrQfPfwJZ"
      },
      "source": [
        "And that's all for this lab. We've seen examples of what each pre-processing step looks like. For our purposes, you can use this code, *ai.clean()*, to take care of the cleaning. If you want to have a closer look, reference that function in the *text_analytics* package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5et2xyBPfwJZ",
        "outputId": "d3fbb87a-17fd-43d5-b4d8-66d77a1e2694"
      },
      "source": [
        "line = ai.print_sample(df)\n",
        "print(line)\n",
        "\n",
        "line_split = line.split()\n",
        "print(\"\\n\")\n",
        "print(\"Line Split\")\n",
        "print(line_split)\n",
        "\n",
        "line_s1 = ai.clean_wordclouds(line, stage = 1)\n",
        "print(\"\\n\")\n",
        "print(\"Line Stage 1\", \"(Remove stop words)\")\n",
        "print(line_s1)\n",
        "\n",
        "line_s2 = ai.clean_wordclouds(line, stage = 2)\n",
        "print(\"\\n\")\n",
        "print(\"Line Stage2\", \"(Make everything lower case)\")\n",
        "print(line_s2)\n",
        "\n",
        "line_s3 = ai.clean_wordclouds(line, stage = 3)\n",
        "print(\"\\n\")\n",
        "print(\"Line Stage3\", \"(Remove punctuation)\")\n",
        "print(line_s3)\n",
        "\n",
        "line_s4 = ai.clean_wordclouds(line, stage = 4)\n",
        "print(\"\\n\")\n",
        "print(\"Line Stage4\", \"(Remove any other non-linguisic material: emails, links)\")\n",
        "print(line_s4)\n",
        "\n",
        "line_s5 = ai.clean_wordclouds(line, stage = 5)\n",
        "print(\"\\n\")\n",
        "print(\"Line Stage5\", \"Join phrases\")\n",
        "print(line_s5)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['How several career diplomats illuminated the state of foreign policy in the Trump administration, even as the White House tried to block them from testifying.']\n",
            "How several career diplomats illuminated the state of foreign policy in the Trump administration, even as the White House tried to block them from testifying.\n",
            "\n",
            "\n",
            "Line Split\n",
            "['How', 'several', 'career', 'diplomats', 'illuminated', 'the', 'state', 'of', 'foreign', 'policy', 'in', 'the', 'Trump', 'administration,', 'even', 'as', 'the', 'White', 'House', 'tried', 'to', 'block', 'them', 'from', 'testifying.']\n",
            "\n",
            "\n",
            "Line Stage 1 (Remove stop words)\n",
            "['How', 'several', 'career', 'diplomats', 'illuminated', 'state', 'foreign', 'policy', 'Trump', 'administration,', 'White', 'House', 'tried', 'block', 'testifying.']\n",
            "\n",
            "\n",
            "Line Stage2 (Make everything lower case)\n",
            "['several', 'career', 'diplomats', 'illuminated', 'state', 'foreign', 'policy', 'trump', 'administration,', 'white', 'house', 'tried', 'block', 'testifying.']\n",
            "\n",
            "\n",
            "Line Stage3 (Remove punctuation)\n",
            "['several', 'career', 'diplomats', 'illuminated', 'state', 'foreign', 'policy', 'trump', 'administration', 'white', 'house', 'tried', 'block', 'testifying']\n",
            "\n",
            "\n",
            "Line Stage4 (Remove any other non-linguisic material: emails, links)\n",
            "['several', 'career', 'diplomats', 'illuminated', 'state', 'foreign', 'policy', 'trump', 'administration', 'white', 'house', 'tried', 'block', 'testifying']\n",
            "\n",
            "\n",
            "Line Stage5 Join phrases\n",
            "['several', 'career', 'diplomats', 'illuminated', 'state', 'foreign', 'policy', 'trump', 'administration', 'white', 'house', 'tried', 'block', 'testifying']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}